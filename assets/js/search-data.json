{
  
    
        "post0": {
            "title": "Docker quick reference",
            "content": "Docker is based on LXC, an operating system level containerization method. Docker offers a high level tool on top of that. | The main purpose of Docker is to package and containerize applications with their dependencies and ship them | Docker uses Namespaces to provide isolation Docker uses cgroups (control groups) to restrict the amount of hardware resources allocated to each container | . | . Common Docker commands . docker run Run a container from an image. If the image is not present on the host, docker will search and download it from the configured registry (DockerHub) | Attached or Detached mode docker run -d image_name | docker attach container_id : Attach a detached container | . | Specify a tag docker run -d redis:4.0 : Here, 4.0 is the tag | If you don&#39;t specify a tag, latest is considered by default | . | . | docker ps List running containers with some basic information | docker ps -a : List ALL containers | . | docker stop container_name_or_id Stop a running container | . | docker rm container_name_or_id Remove a stopped container | . | docker images Show available images | . | docker rmi image_name Remove an image | . | docker pull Only pull an image but don&#39;t run | . | Execute a command on a running container docker exec container_name_or_id cat /etc/hosts | . | Port mapping : map a port from inside the docker container to the host its running on docker run -p 80:5000 tanmay/my-simple-app | The above command will make sure that all traffic on port 80 of your host is routed to port 5000 inside your docker container | . | Data persistance You can map a directory on your host (outside the docker container) to a file system location of your container to make sure the data persists | e.g. Assume that you&#39;re running a mysql container the mysql server stores its data at the location var/lib/mysql. If you remove the container, this data will be lost | you can map a directory on your host instead like so: docker run -v /opt/datadir:/var/lib/mysql mysql | This way when the container runs, it will mount the external directory and store the data there | . | . | . | Inspecting the container (sometimes ps is not enough) docker inspect container_name_or_id | Returns the container configuration in JSON format | . | Logs docker logs container_name_of_id | . | Passing environment variables to docker run docker run -e APP_COLOR=blue simple-web-app | . | List all networks docker network ls | . | Important points . A container lives only until the process inside it is alive | To map the standard input of your host to a container, you need to pass the i flag for the interactive mode docker run -i redis | . | To get the app&#39;s output on your terminal, use the -t flag for the terminal mode docker run -it redis | . | The -it combination makes sure that your container is attached to the terminal as well as running in interactive mode | . Dockerfile . All Dockerfiles must start with the FROM command (every Docker image derives from a base image. Either an OS or an existing app) | Each line in the Dockerfile creates a new layer | All built layers are cached. This allows rebuilds to be fast | Instructions CMD Defines the program that will be run within the container when it starts | The command and its parameters should be separately specified like so : CMD [&quot;sleep&quot;, &quot;5&quot;] and NOT like CMD [&quot;sleep 5&quot;] | You can also specify the command to be run at startup from the command line: docker run ubuntu sleep 10. This will override the command in the Dockerfile | . | ENTRYPOINT This instruction will append to the list of existing commands in the Dockerfile | . | . | . from IPython.display import Image Image(&quot;../images/entrypoint_and_cmd.png&quot;) . Image(&quot;../images/entrypoint_and_cmd_2.png&quot;) . Networking in Docker . When you install docker, it creates 3 networks automatically: Bridge: this is the default network a container gets attached to it&#39;s a private and internal network | All containers attched to Bridge get an internal IP address by default | The containers can access each others using this IP if required | . | none the containers are not attached with any network. They are completely isolated. | . | host if you use the host network, you can directly run a container on the required port, without any port mapping | but this also means that you won&#39;t be able to run multiple instances on the same host on the same port | . | | You can modify the network information using the cli parameter --network docker run ubuntu --network=host | . | Create a network docker network create --driver bridge --subnet 182.18.0.0/16 my-custom-network | . | Containers can reach each others using their names Docker has a built in DNS server and it always runs at 127.0.0.11 | . | . Storage in Docker . When you install docker, it creates the following directory structure . /var/lib/docker aufs | containers | image | volumes | . | . | Adding a persistent volume to a container . docker volume create data_volume (this gets stored in the volumes directory above) | docker run -v data_volume:/var/lib/mysql mysql | If you run the above command w/o first creating a volume, docker will automatically create a volume docker run -v data_volume2:/var/lib/mysql mysql | . | If you want to mount an existing location on the host, use the following command instead (This is called a Bind mount) docker run -v /data/mysql:/var/lib/mysql mysql | . | The more recent syntax is as follows: docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql | . | . | Docker uses readily available storage drivers on the OS to handle all this . AUFS | ZFS | BTRFS | Device Mapper | Overlay | Overlay2 | . | . Docker Compose . Create a configuration file in YAML to create an entire application stack | Can be used on a single host | . Example - a simple web app with UI . Assume that the app is using several different services . A python app called voting-app that has a front end UI | A redis in memory database | A postgreSQL database | A worker written in .NET | A result app that shows the votes | How will this look using plain docker commands? . Note: The following way of Linking together containers is soon going to be deprecated, and will be replaced with better methods. Showing them here just for knowledge. . docker run -d --name=redis redis | docker run -d --name=db postgres:9.4 | docker run -d --name=vote -p 5000:80 --link redis:redis voting-app | docker run -d --name=result -p 5001:80 --link db:db result-app | docker run -d --name=worker --link db:db --link redis:redis worker | Using Docker compose (Version 1) . redis: image: redis db: image: postgres:9.4 vote: image: voting-app ports: - 5000:80 links: - redis result: image: result-app ports: - 5001:80 links: - db worker: image: worker links: - redis - db . Now, use docker-compose up to create the entire stack .",
            "url": "https://tanmay-kulkarni.github.io/blog/docker/2022/06/01/Docker-Primer.html",
            "relUrl": "/docker/2022/06/01/Docker-Primer.html",
            "date": " • Jun 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Serverless Configuration on new machine",
            "content": "The pain of installing Serverless . I&#39;ve always run into permissions/dependency issues when installing serverless on a new machine. First, it needs node installed, which I&#39;m not the least bit familiar with. Then it needs npm which is a package manager. So far, okay. . But the serverless documentation for installing it is not super useful, IMHO, for it&#39;s not super clear about what to do if you simply can&#39;t find your serverless command or fail to install it despite following the tutorial. . Do this instead . ( Thanks to this answer on SO: https://stackoverflow.com/a/66166866/5486022 ) . Step 1: Install Node . Go to https://nodejs.org/en/ and download the node installer for you system (I use Linux) . | $ sudo apt update . | $ sudo apt install xz-utils . | $ sudo tar -xvf name_of_file (let it be node-v14.15.5-linux-x64) . | $ sudo cp -r node-v14.15.5-linux-x64/{bin,include,lib,share} /usr/ . | $ node --version . | | . You should now have node installed on your system . Step 2: Install Serverless as a project dependency (avoid global install npm install -g serverless) . ( Thanks to this answer on SO: https://stackoverflow.com/a/68069795/5486022 ) . $ npm i -D serverless (At your serverless project root) . | $ npx sls --version . | $ npx serverless deploy OR npx sls deploy . | | . You should now have Serverless installed for your project .",
            "url": "https://tanmay-kulkarni.github.io/blog/serverless/lambda/2022/04/06/Serverless-Configuration.html",
            "relUrl": "/serverless/lambda/2022/04/06/Serverless-Configuration.html",
            "date": " • Apr 6, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Airflow",
            "content": "Running a workflow in Airflow . Running a simple Airflow task airflow run &lt;dag_id&gt; &lt;task_id&gt; &lt;start_date&gt; . Dags are written in Python, but they can use components written in other languages - Dags are made up of components (called tasks) to be executed, such as Operators, sensors, etc. . | .",
            "url": "https://tanmay-kulkarni.github.io/blog/airflow/interview-prep/2022/01/12/airflow.html",
            "relUrl": "/airflow/interview-prep/2022/01/12/airflow.html",
            "date": " • Jan 12, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "The Certified Developer Associate Exam",
            "content": "Each region has multiple availability zones (usually 3, min is 2, max is 6) each AZ is one or more discrete data centers with its own redundant power, networking and connectivity | AWS currently has 216 points of presence (205 Edge locations, 11 Regional caches) in 84 cities across 42 countries | . | . IAM . IAM is a global service. Doesn&#39;t require Region selection | A user can belong to multiple groups | In AWS you apply the Least Privilege principle. i.e. you don&#39;t give more permissions than a user needs | . IAM Policy Structure . It consists of: . - Version - Id: optional - Statement: one or more individual statements (required) * Sid: identifier for the statement (optional) * Effect: whether the statement allows or denies access (Allow, Deny) * Principal: account/user/role to which this policy applies * Action: List of actions this policy allows or denies * Resource: List of resources to which this action applies * Condition: conditions for when this policy is in effect (optional) . Multi Factor Authentication (MFA) device options in AWS: . | Virtual MFA device (like Google Authenticator) . | Universal 2nd Factor (U2F) Security Key (A physical device) | Hardware key Fob MFA device | Hardware Key Fob MFA Device for AWS GovCloud (US) | . IAM roles are a secure way of granting permissions to entities you trust, to perform actions on your behalf. Examples of entities are: . IAM user in another account | Application code running on Ec2 that needs to perform actions on AWS resources | An AWS service that needs to act on resources in your account to provide features | Users from a corporate directory who use Identity Federation with SAML | . | IAM security Tools: . IAM Credentials report (account-level) This report lists all your account&#39;s users and status of their various credentials | . | IAM Access advisor shows the service permissions granted to each user and when they were last accessed | . | . EC2 . It is possible to bootstrap instances using an EC2 user data script. It allows you to run commands when your machine starts. That script is only run once at the instance first start. THE EC2 USER DATA SCRIPT RUNS WITH THE ROOT USER. . | Different EC2 instance types are optimised for different use cases. Naming convention: . m5.2xlarge, m: instance class, 5: generation, 2xlarge: size within the instance class | . | Application ports to remember: . 22: SSH | 21: FTP | 22: SFTP (Uploads using SSH) | 80: HTTP | 443: HTTPS | 3389: RDP (Remote Desktop Protocol) - log into Windows instances | . | EBS Volume . It&#39;s a network drive | It can be detached from an EC2 instance and attached to another quickly | It&#39;s locked to an Availability Zone (AZ) | You can copy snapshots across AZ or region | To move a volume across AZ, you&#39;ll need to snapshot it. | An EBS volume cannot be attached to more than 1 EC2 instance at a time | However, an EC2 instance can have more than 1 EBS volumes attached to it at the same time | It is possible to leave EBS volumes unattached | By default, the root EBS volume is deleted | By default, any other attached EBS volume is not deleted | It&#39;s not necessary to detach a volume to do the snapshot, but it&#39;s recommended | . | EC2 Instance Store . EBS volumes are network drives with good but limited performance | If you need a high-performance hardware disk, use EC2 instance store (It has high I/O performance) | EC2 instance store lose their storage when they&#39;re stopped (ephemeral) | EC2 instance store is therefore great for buffer/cache/scrath data/temporary content | . | EBS volume types: . gp2/gp3 (SSD): General Purpose SSD volume that balance price and performance for a wide variety of workloads | io1/io2 (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads | st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput-intensive workloads | sc2 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads | Only gp2/gp3 and io1/io2 can be used as boot volumes | . | . EBS Multi Attach: . Attach the same EBS volume to multiple EC2 instances in the same AZ | Each instance has full read and write permissions to the volume | Use case: Achieve higher application availability in clustered Linux applications (ex - Teradata) | Must use a file system that is cluster-aware | . | EFS - Elastic File System . Managed NFS that can be mounted on many EC2 | EFS works with EC2 instances in multiple Availability Zones (AZ) | It&#39;s highly available, scalable and expensive (3x gp2), pay per use | Compatible with Linux based AMI only (Not Windows) | Encryption at rest using KMS | It has standard file API | Throughput 10GB+/s ... can grow to petabyte-scale network file system, automatically | Supports Performance (default) and Throughput mode | Storage Tiers (lifecycle management feature - move file after N days) - Standard: for frequently accessed files - Infrequently accessed files (EFS-IA): cost to retrieve files, lower price to store . | . | Load Balancer . Classic Load Balancer (v1 - old generation) Supports HTTP, HTTPS, TCP, SSL | Health checks are either TCP based or HTTP based | You&#39;ll get a fixed hostname out of these | Supports only one SSL certificate | . | Application Load Balancer (v2 = new generation) . HTTP, HTTPS, WebSocket | It&#39;s a layer 7 load balancer only | Load balancing to multiple HTTP applications across machines (target groups) | Load balancing to multiple applications on the same machine | Support for HTTP/2 and websocket | Fixed hostname | The application servers don&#39;t see the IP of the client directly The true IP of the client is inserted in the header X-Forwarded-For | We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto) | . | Supports multiple listeners with multiple SSL certificates. It uses Server Name Indication (SNI) to make it work. | Lambda function can be a registered target in a target group of an ALB | You can&#39;t attach an Elastic IP address to Application Load Balancers. | . | Network Load Balancer (v2 - new generation) . TCP, TLS, UDP | Forward TCP &amp; UDP traffic to your instances | Handle millions of requests per seconds | NLBs are used for extreme performance, TCP or UDP Traffic | You can assign 1 fixed IP (static IP) per availability zone to a NLB | Supports multiple listeners with multiple SSL certificates. It uses Server Name Indication (SNI) to make it work. | . | Gateway Load Balancer . Operates at layer 3 (Network Layer) | Deploy, scale and manage a fleet of 3rd party network virtual appliances | If you want all traffic to go through Firewalls, Intrusion Detection and prevention systems, Deep packet inspection, Payload manipulation before it reaches your application, then this LB is used | Uses the GENEVE protocol on port 6081 | . | . | Sticky sessions for a load balancer (Session Affinity) . It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer | This behaviour can be enabled for the CLASSIC Load Balancer and Application Load Balancer | The cookie used for stickiness has an expiration date you control | Use Case: Make sure the user doesn&#39;t lose the session data | The cookies can be Application-based cookie (Custom cookie, generated by the target) Can include custom attributes required by the application | Cookie name must be specified individually for each target group | DON&#39;T use AWSALB, AWSALBAPP or AWSALBTG (reserved for use by the ELB) | . | Application Cookie Generated by the Load Balancer | Cookie name is AWSALBAPP | . | Duration-based Cookie Cookie generated by the load balancer | Cookie name is AWSALB for ALB and AWSELB for CLB | . | . | . | . Cross-Zone balancing . It&#39;s possible to distribute the traffic evenly across multiple AZ irrespective of how many instances there are within that AZ (with Cross-Zone balancing) | It&#39;s also possible to distribute the traffic based on the proportion of instances in each AZ (without Cross-Zone balancing) . | Cross-Zone Load Balancing is Always On in an Application Load Balancer (can&#39;t be disabled. No charges of Inter AZ-data) . | Cross-Zone Load Balancing is disabled by default in the Network Load Balancer. You pay charges for inter AZ data if enabled | Cross-Zone Load Balancing is disabled by default in the classic Load balancer (No charges for inter AZ data if enabled) | . | . SSL/TLS - Basics . SSL = Secure Sockets Layer, used to encrypt connections | TLS = Transport Layer Security, which is a newer version. Nowadays, TLS certificates are mainly used, but people still refer to them as SSL | Public SSL certificates are issued by Certificate Authorities (CA) | The load balancer used an X.509 certificate (or SSL/TLS certificate) | You can manage certificates using AWS Certificate Manager | . SNI - Server Name Indication . SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites) | It&#39;s a newer protocol and requires the client to indicate the hostname of the target server in the initial SSL handshake | The server will then find the correct certificate, or return the default one | It only works with Application Load Balancers (ALB) &amp; Network Load Balancers (NLB) and CloudFront. It does NOT work with Classic Load Balancer (CLB) | . Connection Draining . Known as Connection Draining - For Classic Load Balancer (CLB) | Known as Deregistration Delay - For ALB and NLB | The idea is that it will give some time to your instances to complete the In Flight Requests, while the instance is being deregisterd or marked unhealthy | Once the instance is drained, it will stop sending new requests to the EC2 instance which is being de-registered | . Auto Scaling Group (ASG) . An ASG contains . AMI + Instance Type | EC2 user data | EBS volumes | Security Groups | SSH Key Paid | . | You can set Min Size/ Max Size / Initial Capacity . | Network + Subnets Information | Load Balancer Information | Scaling Policies | . ASG Policies . Target Tracking Scaling Most simple and easy to set-up | Example: I want the average ASG CPU to stay at around 40% | . | Simple/Step scaling When a cloudwatch alarm is triggered, e.g. CPU &gt; 70%, then add 2 units | When a cloudwatch alarm is triggered, e.g. CPU &lt; 30%, then remove 1 unit | . | Scheduled Actions Anticipate a scaling based on usage patterns | . | Predictive scaling Forecast usage demand using ML and scale ahead | . | . Good metrics to scale on . 1. CPUUtilization: Avg CPU utilization across your insances 2. RequestCountPerTarget: To make sure the number of requests per EC2 instances is stable 3. Avg Network In/Out 4. Any custom metric that you can push using CloudWatch . Auto Scaling Groups - Cooldowns . After a scaling activity happens, you are in a cooldown period (default 300 seconds) During the cooldown period, the ASG will not launch or terminate additional instances | . | . Relational Database Service (RDS) . You can&#39;t SSH into your an RDS instance | Automated Backups Daily full backup of the database (during the maintenance window) | Transaction logs are backed-up by RDS every 5 minutes Ability to restore to any point in time from oldest backup to 5 minutes ago | . | . | DB Snapshots . Manually triggered by the user | Retention of backup for as long as you want | . | RDS - Storage Auto Scaling . When RDS detects you are running our of free database storage, it scales automatically | You have to set maximum storage theshold (maximum limit for DB storage) | . | . RDS Read Replica . - You can create upto 5 Read Replicas - Read Replicas can be within AZ, Cross AZ or Cross Region - Relication is ASYNCHRONOUS... so reads are eventually consistent - Replicas can be promoted to their own DB - Read replicas are used only for SELECT (=read) kind of queries. No Insert, Update, Delete is allowed - In AWS generally there&#39;s a cost when data goes from one AZ to another. **But for RDS Read Replicas, within the same region, you don&#39;t pay that fee.** . RDS Multi AZ (Disaster Recovery) . - Synchronous replication to a standby instance - One DNS name -- automatic failover to standby - Failover in case of loss of AZ, loss of network, instance or storage failure - No manual intervention is needed in apps - Note: Read Replicas can be setup as Mutli AZ for disaster recovery . RDS Security - Encryption . * At rest encrypption - Possibility to encrypt the master and read replicas with AWS KMS - AES-256 encryption - Encryption has to be defined at launch time - If the master is not encrypted, the read replicas cannot be encrypted - You can also enable Transparent Data Encryption (TDE) available for Oracle and SQL server * In-flight encryption - SSL certificates to encrypt data to RDS in flight - Provide SSL options with trust certificate when connecting to database - To enforce SSL: * PostgreSQL: rds.force_ssl=1 in the AWS RDS Console (Parameter Groups) * MySQL: Within the DB: GRANT USAGE ON *.* TO &#39;mysqluser&#39;@&#39;%&#39; REQUIRE SSL; * Encrypting RDS backups - Snapshots of un-encrypted RDS databases are un-encrypted - Snapshots of encrypted RDS databases are encrypted * To encrypt an un-encrypted RDS database: - Create a snapshot of the un-encrypted database - Copy the snapshot and enable encryption for the snapshot - Restore the database from the encrypted snapshot - Migrate the application to the new database, and delete the old database . Aurora High Availability and Read Scaling . * 6 copies of your data across 3 AZ - 4 copies out of 6 are needed for writes - 3 copies out of 6 are needed for reads * Automated failover for master in less than 30 seconds * You can have Master + upto 15 Aurora Read Replicas (have to choose while creating the DB) * Support Cross Region Relication * Aurora provides - Writer Endpoints --&gt; An Endpoint that always points to the master - Reader Endpoints --&gt; An Endpoint for READ requests. Takes care of autoscaling and loadbalancing Read Replicas . Elasticache . * Managed service for Redis and Memcached * In memory databases with really high performance, low latency * Helps reduce load off of databases and read intensive workloads * AWS takes care of OS maintenance/patching, optimizations, setup, configuration, failover recovery and backups * REDIS - Multi AZ with Auto-Failover - Read Replicas to scale reads and have high availability - Backup and restore features * MEMCACHED - Multi-node for partitioning of data (sharding) - No high availability (replication) - Non persistent - No backup and restore - Multi-threaded architecture * Caching Strategies - Lazy Loading / Cache-Aside / Lazy Population * Pros: 1. Only requested data is cached (cache doesn&#39;t have unused data) 2. Node failovers are not fatal * Cons: 1. Cache miss penalty that results in 3 round trips 2. Stale data: Data can be updated in the database but outdated in the cache - Write Though: Add or update cache when database is updated * Pros: 1. Data in cache is never stale 2. Write penalty requires 2 calls as compared to 3 calls for cache miss penalty in Lazy loading * Cons: 1. Missing data until it&#39;s added/updated in the cache 2. A lot of data in the cache may not be read * Cache Evictions and Time-to-live (TTL) * Cache eviction can occur in three ways: 1. You delete the item explicitly from the cache 2. Item is evicted because the memory is full and it&#39;s not recently used (LRU) 3. You set an item time-to-live (TTL) . Route 53 . - The only service AWS provides that has 100% SLA - Record Types: * A - maps a hostname to IPv4 * AAAA - maps a hostname to IPv6 * CNAME - maps a hostname to another hostname * NS - Name Servers for the Hosted Zone . CNAME vs Hostname . - Route 53 - Alias Records - AWS Resources (Load Balancer, Cloudfront) expose an AWS hostname... but you want myapp.mydomain.com - CNAME points a hostname to any other hostname... only works for non root domain - Alias: - points a hostname to an AWS resource - works for ROOT DOMAIN and NON ROOT DOMAIN - An extension of the DNS functionality - Automatically recognizes changes in the resource&#39;s IP address - Alias record is always of type A/AAAA for AWS resources - Route 53 - Alias Records Target 1. CloudFront Distributions 2. API Gateway 3. Elastic Beanstalk 4. S3 Websites 5. VPC Interface Endpoints - **You CANNOT set an ALIAS record for an EC2 DNS name** . Route 53 - Routing Policies . It defines how Route 53 responds to DNS queries (Not to be confused with the word routing.. DNS doesn&#39;t route any traffic, it only responds to DNS queries) | Route 53 supports the following Routing policies: Simple | Weighted | Failover | Latency based | Geolocation | Multi-value answer | Geoproximity | . | . Virtual Private Cloud (VPC) . A private network to deploy your resources within the cloud | VPC is a regional resource | Subnets allow you to partition your network inside your VPC (Subnet is an AZ resource) | A public subnet is a subnet that is accessible from the internet. | Internet Gateways help our VPC instances connect with the internet. Public subnets have a route to the internet gateway | NAT Gateways (AWS-managed) &amp; NAT instances (self-managed) allow your instances in your Private Subnets to access the internet while remaining private | . from IPython.display import Image Image(&quot;/home/tanmay/Pictures/training/vpc_nat.png&quot;) . Network ACL and Security groups . A firewall which controls traffic from and to subnet | Can have allow or deny rules | Rules only include IP addresses | The default NACL allows everything IN and everything OUT . | Security group is a firewall that controls traffic to and from an ENI/EC2 instance . | Can have only allow rules | Rules include IP addreses and other security groups | . Image(&quot;/home/tanmay/Pictures/training/nacl_vs_sg.png&quot;) . VPC Flow Logs . You can capture information about the IP traffic flowing through your interfaces . VPC Flow logs . | Subnet Flow Logs . | Elastic Network Interface Flow Logs . | . | . VPC Peering . Connect two VPC, privately using AWS&#39; network | Make them behave as if they were in the same network | Must not have overlapping CIDR (IP address range) | VPC peering connection is not transitive (must be established for each VPC that need to communicate with each other) | . VPC Endpoints . Endpoints allow you to connect to AWS services from within your VPC using a private network instead of the public www network | This gives you enhanced security and lower latency to acces AWS services | VPC Endpoint Gateway: This is only for S3 and DynamoDB | VPC Endpoint Interface: The rest of the services | . Image(&quot;/home/tanmay/Pictures/training/vpc_endpoints.png&quot;) . Site to site VPN and Direct Connect . Connect an on-premises VPN to AWS | The connection is automatically encrypted | Goes over the public internet | . Direct Connect . Establish a physical connection between on-premises and AWS | The connection is private, secure and fast | Goes over a private network | Takes some time to establish (~1 month) | . NOTE: . Site-to-site VPN and Direct Connect cannot access VPC endpoints . VPC Summary . Image(&quot;/home/tanmay/Pictures/training/vpc_summary.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/three_tier_architecture.png&quot;) . S3 . Each bucket must have a globally unique name | Maximum object size on S3 is 5 TB (5000GB) | If uploading more than 5GB, must use &quot;multi-part upload&quot; | Each object in S3 can have metadata | Any file that is not versioned prior to enabling versioning will have version &quot;null&quot; | Suspending versioning does not delete the previous versions | . S3 Encryption . - There are 4 methods of encrypting objects in S3 * SSE-S3: Encrypts S3 objects using keys handled &amp; managed by AWS - AES-256 encryption - Must set header: &quot;x-amz-server-side-encryption&quot;: &quot;AES256&quot; * SSE-KMS: Leverage AWS Key Management Service to mange encryption keys - KMS Advantages: user control over who has access to keys + Audit trail - Must set header: &quot;x-amz-server-side-encryption&quot; : &quot;aws:kms&quot; * SSE-C: When you want to manage your own encryption keys - Server side encryotion using data keys fully managed by the customer outside of AWS - AWS S3 does not store the encryption key you provide - HTTPS must be used - Encryption key must be provided in HTTP headers, for every HTTP request made * Client Side Encryption - You encrypt data before you upload it onto S3 - Some client libraries can help you do this: ex. AWS S3 Encryption Client - Clients must decrypt data themselves when retrieving from S3 . Image(&quot;/home/tanmay/Pictures/training/aws-encryption-sse-s3.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/aws-encryption-sse-kms.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/aws-encryption-sse-c.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/aws-encryption-sse-client-side.png&quot;) . S3 Security . User Based IAM policies: which API calls should be allowed for a specific user from IAM console | . | Resource Based Bucket policies: bucket wide rules from the S3 console -- allows cross account | Object Access Control List (ACL) -- finer grain | Bucket Access Control List (ACL) -- less common | . | . NOTE . - An IAM principal (it could be a user or a role) can access an S3 object, if the IAM policy allows it OR the Resource policy allows it - AND there&#39;s no explicit DENY . S3 Bucket Policies . JSON based | Use S3 bucket policies to Grant public access to a bucket | Force objects to be encrypted at upload etc | Grant access to another account (using Cross account S3 bucket policies) | . | . Bucket settings for Block Public Access . - Block public access to buckets and objects granted through * new access control lists (ACLs) * any access control lists (ACLs) * new public bucket or access point policies . Other things to note in S3 security . Networking Supports VPC endpoints (So that instances in VPC can connect privately without the public internet) | . | Logging and Audit S3 Access Logs can be stored in other S3 bucket | API calls can be logged in CloudTrail | . | User security MFA Delete: MFA can be required in versioned buckets to delete objects | Pre-signed URLs: URLs that are valid only for a limited time | . | . | . Image(&quot;/home/tanmay/Pictures/training/s3_websites.png&quot;) . CORS . - When you visit a website, make requests to other origins ONLY IF the other origins allow it . Image(&quot;/home/tanmay/Pictures/training/cors.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cors_example.png&quot;) . Amazon S3 consistency model . As of December 2020: S3 supports strong consistency After successful write of new object or overwrite or delete of an existing object, subsequent reads immediately receive the latest version of the object (Read after Write consistency) | Subsequent list requests immediately reflects changes | . | . AWS CICD . Image(&quot;/home/tanmay/Pictures/training/ci.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cd.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/code_pipeline.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/codebuild.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/code_artifact.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/codeguru.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cloudformation_resources.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cloudformation_faq.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_parameters.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_parameters_ref.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_pseudo_parameters.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_mappings.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_outputs.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_cross_stack_ref.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_conditions.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_how_to_define_condition.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_using_conditions.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_must_know_for_exam.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_Ref.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_GetAtt.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_mapping_syntax.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_join.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_sub.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_rollbacks.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_change_sets.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_nested_stack.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_cross_stack_vs_nested.png&quot;) . Image(&quot;/home/tanmay/Pictures/training/cf_stacksets.png&quot;) . ECS . Launch Docker containers on AWS = Launch ECS tasks on ECS clusters | With EC2 launch type, you must provision and maintain the EC2 instances | With Fargate launch type, you do not need to provision the infrastructure (no EC2 instances to manage). It&#39;s serverless! | ECS task role allows each task to have a specific role | Task role is defined in the task definition | ECS can integrate with Load Balancers Application Load Balancer - supported and works for most cases | Network Load Balancer - recommended only for high throughput / high performance use cases or to pair it with AWS private link | Elastic Load Balancer - supported but not recommended. No advance features - No fargate | . | . ECS - Auto scaling . Automatically increase/decrease the desired number of ECS tasks | AWS ECS Auto sclaing uses AWS Application Auto Scaling ECS service average CPU utilization | ECS service average memory utilization | ALB request count per target - metric coming from the ALB | . | Target Tracking - scale based on target value for a specific CloudWatch metric | Step Scaling - scale based on a specified CloudWatch alarm | Scheduled scaling - scale based on a specified date/time (when changes are predictable) | ECS service Auto scaling (task level) != EC2 Auto Scaling (EC2 instance level) | Fargate auto scaling is much easier to setup | . EC2 launch type - Auto scaling EC2 instances . Accommodate ECS service scaling by adding underlying EC2 instances | Add EC2 instances over time | . ECS Cluster Capacity Provider . Used to automatically provision and scale the infrastructure for your ECS tasks | Capacity provider paird with an Auto scaling group | Add EC2 instances when you&#39;re missing capacity | . ECS - Task definitions . Tasks definitions are metadata in JSON format that tell ECS how to run a Docker container | It contains critical information such as - Image Name - Port binding for container and host - Memory and CPU required - Environment Variables - Networking information - IAM role - Logging configuration . | . ECS Rolling updates . from IPython.display import Image Image(&quot;../images/ecs_rolling_updates.png&quot;) . ECS tasks can be invoked from Event Bridge . Image(&quot;../images/ecs_tasks_from_eventbridge.png&quot;) . ECS tasks invoked on Eventbridge schedule . Image(&quot;../images/ecs_invoked_on_eventbridge_schedule.png&quot;) . One IAM role is created per task definition . Image(&quot;../images/one_iam_role_per_task_definition.png&quot;) . ECS environment variables . Image(&quot;../images/ecs_env_variables.png&quot;) . Data volumes . Image(&quot;../images/ecs_data_volumes.png&quot;) . ECS task placements . Image(&quot;../images/ecs_task_placement.png&quot;) . Image(&quot;../images/ecs_task_placement_process.png&quot;) . Image(&quot;../images/task_placement_strategy_binpack.png&quot;) . Image(&quot;../images/task_placement_strategy_random.png&quot;) . Image(&quot;../images/task_placement_strategy_spread.png&quot;) . Image(&quot;../images/task_placement_strategy_mix.png&quot;) . from IPython.display import Image Image(&quot;../images/ecr_commands.png&quot;) . API Gateway . specify a Lambda alias to trigger on API Gateway . lambda-name:${stageVariables.lambdaAlias} | . | . After specifying this, Lambda console promots you to set/update proper permissions to use stageVariables. . Of course, in each stage (setting found in the AWS Lambda console), you have to setup a variable called lambdaAlias . Canary deployment . Choose the percentage of traffic the canary channel receives | Metrics and logs are separate for canary deployments | . Integration Types . Mock API gateway sends a response without sending the request to the backend first | . | AWS/HTTP (Lambda &amp; AWS services) You must configure both the integration request and integration response | . | AWS_PROXY (LAMBDA PROXY) Incoming request from the client is the input to Lambda | The function is responsible for the logic of request/response | No mapping templates/headers/query string parameters ... are passed as arguements | . | HTTP_PROXY No mapping template | The HTTP request is passed to the backend | The HTTP response from the backend is forwarded to the API Gateway | . | API Gateway Cache invalidation . Able to flush the entire cache immediately | Clients can invalidate the cache with header : Cache-Control: max-age=0 (with proper IAM authorization) | If you don&#39;t impose an InvalidateCache policy (or choose the Require authorization check box in the console) any client can invalidate the API cache | . | API Gateway can send logs to CloudWatch logs . for that we need to enable CloudWatch logging at the stage level | Can override these settings per API basis | Log contains information about request/response body | . | X-ray can be enabled | . | CloudWatch metrics CacheHitCount | CacheMissCount | Count : The total number of API requests in a time period | IntegrationLatency : The time between when the API gateway relays a request to the backend and receives a response from the backend | Latency : The time between when the API Gateway receives a request from the client and when it returns a response to the client | . | 4XXError : Client-side errors | 5XXError : Server-side error | . API throttling . Account limit API gateway throttles at 10,000 rps across all APIs | This is a soft limit that can be increased upon request | . | In case of throttling, we get Error 429 too many requests | Can set stage limit and method limit | Or you can define usage plans to throttle per customer | . CORS . CORS must be enabled to receive API calls from another domain | The OPTIONS pre-flight request must contain the following headers Access-Control-Allow-Methods | Access-Control-Allow-Headers | Access-Control-Allow-Origin | . | Cors can be enabled through console | . HTTP API vs REST API . HTTP API is a low cost alternative to the REST API supported by AWS | Only supports proxy integrations | There are no usage plans and API keys | . Websocket API . from IPython.display import Image . Image(&quot;../images/websocket.png&quot;) . Websocket connection example | . Image(&quot;../images/connecting_to_the_websocket_API.png&quot;) . Websocket Routing | . Image(&quot;../images/websocket_routing.png&quot;) . DynamoDB . It&#39;s a NoSQL serverless database | NoSQL databases genrally do NOT support join queries (or have very limited support for it) | NoSQL databases don&#39;t perform aggregation such as SUM , AVG | NoSQL databases scale horizontally | It&#39;s made up of tables: Each table has a primary key | Each table can have infinite number of items (rows) | Each item has attributes | Max size of an item is 400 KiB | . | DynamoDB supports a number of datatypes: Scalar Types: String, Boolean, Number, Binary etc. | Document Types: List, Map | Set Types: String set, Number set, Binary set | . | DynamoDB Primary Keys Option 1: Partition Key (Hash) Partition Key must be unique for each item | Partition key must be diverse so that the data is properly distributed | e.g. UserID | . | Option 2: Partition Key + Sort Key (Hash + Range) The combination must be unique for each item | Data is grouped on partition key | e.g. UserID for Partition Key AND GameID for Sort Key | . | . | DynamoDB - Read/Write Capacity Modes Provisioned mode (default) | On-Demand mode (Read/writes automatically scale up/down with your workloads.. so no capacity planning is needed.. This way you pay only for what you use, but it&#39;s more expensive) | You can switch between the two modes once every 24 hours | . | . WCU (Write Capacity Unit) calculations . 1 WCU = 1 write per second for an item up to size 1KB in size | If items are larger than 1KB, more WCU are consumed . | Example 1 . We write 10 items per second of capacity 2KB each. How many WCU are needed? . 10 * (2KB/1KB) = 20 WCU | . | . | Example 2 . We write 6 items per second, with item size 4.5KB each . 6 (4.5KB/1KB) ~ 6 (5KB/1KB) = 30 WCU | Hint: Round Up | . | . | Example 3 . We write 120 items per minute, with items size 2KB . (120/60) * (2KB/1KB) = 4 WCU | . | . | . RCU (Read Capacity Unit) calculations . Eventually Consistent Read (default) If we read just after the write, it&#39;s possible that we will get stale data | . | Strongly Consistent Read . If we read just after the write, we will get the correct data | To enable this, you have to set &quot;ConsistentRead&quot; parameter to True in API calls (GetItem, BatchGetItem, Query, Scan) | Consumes TWICE the RCU | . | 1 RCU represents 1 Strongly Consistent Read per second, or 2 Eventually Consistent Reads per second for an item 4 KB in size . | Example 1 . 10 Strongly Consistent Reads per second, with an item size 4KB each . (10 / 1) * (4KB/4KB) = 10 RCU | . | . | Example 2 . 16 Eventually Consistent Reads per second, with item size 12KB each . (16/2) (12KB/4KB) = 83 = 24 RCU | . | . | Example 3 . 10 Strongly Consistent Reads per second, with item size 6KB . (10/1) (6KB/4KB) = 10 (8KB/4KB) = 20 RCU | . | . | .",
            "url": "https://tanmay-kulkarni.github.io/blog/certification/aws/2022/01/05/aws-cda-notes.html",
            "relUrl": "/certification/aws/2022/01/05/aws-cda-notes.html",
            "date": " • Jan 5, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "AWS KMS",
            "content": "Customer Master Keys (CMK) - The primary resources in AWS KMS are customer master keys (CMKs). Typically, you use CMKs to protect data encryption keys (or data keys) which are then used to encrypt or decrypt larger amounts of data outside of the service. CMKs never leave AWS KMS unencrypted, but data keys can. AWS KMS does not store, manage, or track your data keys. There is one AWS-managed CMK for each service that is integrated with AWS KMS. When you create an encrypted resource in these services, you can choose to protect that resource under the AWS-managed CMK for that service. This CMK is unique to your AWS account and the AWS region in which it is used, and it protects the data keys used by the AWS services to protect your data. . Data keys - Data keys are used to encrypt large data objects within an application outside AWS KMS. . Key rotation and Backing Keys - When you create a customer master key (CMK) in AWS KMS, the service creates a key ID for the CMK and key material, referred to as a backing key, that is tied to the key ID of the CMK. If you choose to enable key rotation for a given CMK, AWS KMS will create a new version of the backing key for each rotation. It is the backing key that is used to perform cryptographic operations such as encryption and decryption. Automated key rotation currently retains all prior backing keys so that decryption of encrypted data can take place transparently. CMK is simply a logical resource that does not change regardless of whether or of how many times the underlying backing keys have been rotated. .",
            "url": "https://tanmay-kulkarni.github.io/blog/certification/kms/aws/2022/01/03/aws-kms.html",
            "relUrl": "/certification/kms/aws/2022/01/03/aws-kms.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "My tmux.conf file",
            "content": "unbind C-b set-option -g prefix C-a bind-key C-a send-prefix bind -Tcopy-mode-vi v send -X begin-selection # split panes using | and - bind | split-window -h bind - split-window -v unbind &#39;&quot;&#39; unbind % # switch panes using Alt-arrow without prefix bind -n M-Left select-pane -L bind -n M-Right select-pane -R bind -n M-Up select-pane -U bind -n M-Down select-pane -D # Enable mouse mode (tmux 2.1 and above) set -g mouse on bind r source-file ~/.tmux.conf # don&#39;t rename windows automatically set-option -g allow-rename off ###################### ### DESIGN CHANGES ### ###################### # loud or quiet? set -g visual-activity off set -g visual-bell off set -g visual-silence off setw -g monitor-activity off set -g bell-action none # modes setw -g clock-mode-colour colour5 setw -g mode-style &#39;fg=colour1 bg=colour18 bold&#39; # panes set -g pane-border-style &#39;fg=colour19 bg=colour0&#39; set -g pane-active-border-style &#39;bg=colour0 fg=colour9&#39; # statusbar set -g status-position bottom set -g status-justify left set -g status-style &#39;bg=colour18 fg=colour137 dim&#39; set -g status-left &#39;&#39; set -g status-right &#39;#[fg=colour233,bg=colour19] %d/%m #[fg=colour233,bg=colour8] %H:%M:%S &#39; set-option -g status-right &quot;#{network_bandwidth}&quot; set -g status-right-length 50 set -g status-left-length 20 setw -g window-status-current-style &#39;fg=colour1 bg=colour19 bold&#39; setw -g window-status-current-format &#39; #I#[fg=colour249]:#[fg=colour255]#W#[fg=colour249]#F &#39; setw -g window-status-style &#39;fg=colour9 bg=colour18&#39; setw -g window-status-format &#39; #I#[fg=colour237]:#[fg=colour250]#W#[fg=colour244]#F &#39; setw -g window-status-bell-style &#39;fg=colour255 bg=colour1 bold&#39; # messages set -g message-style &#39;fg=colour232 bg=colour16 bold&#39; set -g @plugin &#39;tmux-plugins/tpm&#39; set -g @plugin &#39;tmux-plugins/tmux-sensible&#39; set -g @plugin &#39;dracula/tmux&#39; set -g @plugin &#39;tmux-plugins/tmux-resurrect&#39; set -g @plugin &#39;tmux-plugins/tmux-continuum&#39; set -g @plugin &#39;CrispyConductor/tmux-copy-toolkit&#39; set -g @plugin &#39;xamut/tmux-network-bandwidth&#39; # set -g @plugin &#39;tmux-plugins/tmux-yank&#39; # set 256 colors set -s default-terminal &#39;tmux-256color&#39; run &#39;~/.tmux/plugins/tpm/tpm&#39; .",
            "url": "https://tanmay-kulkarni.github.io/blog/tmux/work-environment/2021/10/17/my-tmux-conf.html",
            "relUrl": "/tmux/work-environment/2021/10/17/my-tmux-conf.html",
            "date": " • Oct 17, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Python Tricks",
            "content": "Adding underscores to numbers . num1 = 10000000000 num2 = 90000000 sum1 = num1 + num2 print(sum1) . 10090000000 . num1 = 10_000_000_000 num2 = 90_000_000 sum2 = num1 + num2 print(sum2) . 10090000000 . sum1 == sum2 . True . More readability . print(f&#39;{sum2:,}&#39;) . Note that the type of the output gets changed, obviously . print(type(f&#39;{sum2:,}&#39;)) . &lt;class &#39;str&#39;&gt; . Use context manager to manage file handling . f = open(&#39;myfile.csv&#39;, &#39;r&#39;) file_contents = f.read() f.close() . Try a context manager instead. This way you don&#39;t have to remember to close the file handle . with open(&#39;myfile.csv&#39;, &#39;r&#39;) as f: file_contents = f.read() . The above concept of context managers is applicable not only to files, but to any other resource you might use such as threads. . Using the enumerate function . words = [&#39;A&#39;, &#39;and&#39;, &#39;B&#39;, &#39;play&#39;, &#39;God of war&#39;, &#39;together&#39;] index = 1 for word in words: print(index, word) index += 1 . 1 A 2 and 3 B 4 play 5 God of war 6 together . There&#39;s a better way to do this . for ix, word in enumerate(words, start=1): print(ix, word) . 1 A 2 and 3 B 4 play 5 God of war 6 together . Using the ZIP function . persons = [&#39;Bruce Wayne&#39;, &#39;Clark Kent&#39;, &#39;Peter Parker&#39;] heroes = [&#39;Batman&#39;, &#39;Superman&#39;, &#39;Spiderman&#39;] universes = [&#39;DC&#39;, &#39;DC&#39;, &#39;Marvel&#39;] for person, hero, universe in zip(persons, heroes, universes): print(f&#39;{hero} is actually {person} created by {universe}&#39;) . Batman is actually Bruce Wayne created by DC Superman is actually Clark Kent created by DC Spiderman is actually Peter Parker created by Marvel . Quick note about unpacking values . The following two lines of code result in a ValueError . a,b,c = (1,2,3,4,5) a,b,c,d,e = (1,2,3) . Unpacking fewer variables . a, b, *c = (1,2,3,4,5) print(f&#39;a = {a}&#39;) print(f&#39;b = {b}&#39;) print(f&#39;c = {c}&#39;) . a = 1 b = 2 c = [3, 4, 5] . Accepting passwords . from getpass import getpass username = input(&quot;Username: &quot;) password = getpass(&quot;Password: &quot;) print(&quot;Logging in...&quot;) . Logging in... . That&#39;s it for now! .",
            "url": "https://tanmay-kulkarni.github.io/blog/python/tricks/2021/09/13/Test-from-pop-os.html",
            "relUrl": "/python/tricks/2021/09/13/Test-from-pop-os.html",
            "date": " • Sep 13, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Things to remember about AWS DynamoDB",
            "content": "Types of Primary Keys . 1. Partition Key 2. Composite Key (Partition key + Sort Key) . Access control to Data using IAM . You can give fine-grained access to users in DynamoDB. . e.g. users can access only items where the partitionKey value matches their User_Id . Indexes . A secondary index allows you to perform more flexible querying on DynamoDB. It allows you to query on an attribute which is not the primary key. This can be done using . Global Secondary Index | Local Secondary Index | . You select the columns you want to include in the index and run your searches on the index instead of the entire dataset. . Local Secondary Index . Limitation: Can only be created when you&#39;re creating the table. . Has the same partition key as your original table, but a different sort key. . Global Secondary index . Advantage: Can be created even after creation of the table. It allows you to use a different partition key as well as the sort key than your main table Gives a totally different view of the table than the original table created (because of the above point) . Scan and Query API calls . QUERY . A query operation finds items in your table based on the primary key attribute, and a distinct value to search for. A query can be refined by using an optional sort key name and value. By default a query returns all the attributes. You can limit the attributes to the specific attributes you want by using the ProjectionExpression parameter. . Results are always sorted by the sort key. . You can reverse the order of the result by setting the ScanIndexForward attribute to False. . REMEMBER: ScanIndexForward is applicable only to queries and NOT to scans, despite the name . By default all queries are eventually consistent. . SCAN . A scan operation examines every attribute in the table. Again, you can use the ProjectionExpression parameter to limit the attributes returned. . A query is more efficient than Scan. . Improving performance . - Set smaller page size - Avoid scans if possible - By using parallel scan. A scan by default operates in a sequential manner. But you can configure your DynamoDB to use parallel scans by logically dividing a table or index into segments and scanning each segment in parallel. - Isolate scan operations to specific tables and segregate them from mission critical data . Provisioned Throughput . Provisioned Throughput is measured in capacity units. . Write capacity units 1 x 1KB write per second | . | Strongly consistent reads 1 x 4KB read per second | . | Eventually consistent reads (default option) 2 x 4KB read per second | . | . Example 1: . Imagine a table that has 5 read capacity units and 5 write capacity units . This configuration will be able to perform . 5 x 4KB strongly consistent reads = 20KB per second | Twice as many eventually consistent reads = 40KB per second | 5 x 1KB writes = 5KB writes | . Example 2: . Imagine you have an application that needs to read 80 items per second. Each item is 3KB in size and you need strongly consistent reads. How many read capacity units will you need? . Ans: Number of 4KB read capacity units needed per item = 3KB/4KB = 0.75 ~= 1 Number of items = 80 Therefore read capacity units needed = 80 x 1 = 80 for STRONGLY CONSISTENT What if you need Eventually consistent reads? = #Strongly Consistent / 2 = 80/2 = 40 for EVENTUALLY CONSISTENT . Example 3: . Imagine you need to write 100 items per second to your DynamoDB table. Each item size 512 bytes. Each write capacity unit gives 1 1KB write per second. How many write capacity units will you need to provision? . Ans: - Number of write capacity of 1KB/sec needed per item = 512/1024 bytes = 0.5 ~= 1 write capacity unit per item - Number of writes required per second = 100 - Therefore, number of write capacity units required = 100 x 1 write capacity unit per item = 100 . Provisioned Throughput Exceeded Exception . Occurs when your request rate is too high for the read/write capacity provisioned for your table. . Exponential Backoff . The requester used progressively longer waits between consecutive wait times. . DynamoDB On Demand Capacity . A pricing model for DynamoDB. Charges will be based on the activity. Can autoscale read/write capacity units as needed. . Great for: - Unpredictable workload - New applications where the use pattern is not known yet - When you want to pay for only what you use . DynamoDB Accelerator (DAX) . It&#39;s a fully managed, clustered in memory cache for DynamoDB. . But only for Read performance. Ideal for read heavy or bursty read applications. DAX is a write through caching service. Data is written to the cache and the backend store at the same time This allows you to point your DynamoDB API calls at the DAX cluster *Limitations* 1. It caters to applications that need Eventual Consistency. Not suitable for Strongly consistent read needs 2. Not suitable for write-intensive applications . DynamoDB TTL . Defines an expiry time for your data. Expired items are marked for deletion. . Really good for applications that generate irrelevant or old data. e.g. session data, event logs, any temporary data. . DynamoDB Streams . it&#39;s an ordered sequence of item level modifications. . These are stored as logs. These logs are encrypted at rest and stored for 24 hours only. . They can be used for triggering events based on certain transactions. Great for serverless architectures. . They can also be used for replicating data across multiple tables. .",
            "url": "https://tanmay-kulkarni.github.io/blog/aws/certification/dynamodb/2021/04/06/About-DynamoDB.html",
            "relUrl": "/aws/certification/dynamodb/2021/04/06/About-DynamoDB.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "About AWS X-ray",
            "content": "1. X-ray integrates with many AWS services like DynamoDB, Lambda, API Gateway, etc. 2. You can also instruct your own applications to send data to X-ray 3. applications can be running on EC2, Elastic Beanstalk environments, on-premise systems or ECS 4. For ECS, run the X-ray daemon in it&#39;s own Docker image, running alongside your application 5. You can record application specific information in the form of key-value pairs . You need three things . 1. X-ray SDK 2. X-ray daemon 3. Instruct the application using the SDK to send data to X-ray .",
            "url": "https://tanmay-kulkarni.github.io/blog/aws/certification/aws-x-ray/2021/04/06/AWS-XRay.html",
            "relUrl": "/aws/certification/aws-x-ray/2021/04/06/AWS-XRay.html",
            "date": " • Apr 6, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "iPython tricks",
            "content": "Output of a command can be assigned to a variable . This works only in IPython or Jupyter, it will not run in regular Python | . | . ls = !ls . type(ls) . IPython.utils.text.SList . Commands can be passed to a python interpreter . - python -c &quot;import datetime;print(datetime.datetime.utcnow())&quot; . !df -h . Filesystem Size Used Avail Capacity iused ifree %iused Mounted on /dev/disk1s1s1 466Gi 14Gi 337Gi 4% 568975 4881883905 0% / devfs 230Ki 230Ki 0Bi 100% 796 0 100% /dev /dev/disk1s5 466Gi 6.0Gi 337Gi 2% 6 4882452874 0% /System/Volumes/VM /dev/disk1s3 466Gi 371Mi 337Gi 1% 1298 4882451582 0% /System/Volumes/Preboot /dev/disk1s6 466Gi 2.1Mi 337Gi 1% 15 4882452865 0% /System/Volumes/Update /dev/disk1s2 466Gi 108Gi 337Gi 25% 675547 4881777333 0% /System/Volumes/Data map auto_home 0Bi 0Bi 0Bi 100% 0 0 100% /System/Volumes/Data/home pCloud.fs 8.0Gi 1.9Gi 6.1Gi 25% 0 0 100% /Users/tanmay/pCloud Drive /Users/tanmay/Downloads/TextMate.app 466Gi 104Gi 346Gi 24% 645214 4881807666 0% /private/var/folders/nh/wr9z17hd7kdd2wn87xjknv_40000gn/T/AppTranslocation/A1844839-7447-4F97-8106-9798FF76A7B3 .",
            "url": "https://tanmay-kulkarni.github.io/blog/python/2021/04/04/general-python.html",
            "relUrl": "/python/2021/04/04/general-python.html",
            "date": " • Apr 4, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Git Quick Reference",
            "content": "File diff . To compare the state of your files with those in the staging area, you can use git diff -r HEAD. The -r flag means &quot;compare to a particular revision&quot;, and HEAD is a shortcut meaning &quot;the most recent commit&quot;. . git diff -r HEAD | . Seeing changes to a specific file . A project&#39;s entire log can be overwhelming, so it&#39;s often useful to inspect only the changes to particular files or directories. You can do this using git log path, where path is the path to a specific file or directory. The log for a file shows changes made to that file; the log for a directory shows when files were added or deleted in that directory, rather than when the contents of the directory&#39;s files were changed. .",
            "url": "https://tanmay-kulkarni.github.io/blog/git/2021/03/21/Git-Quick-Reference.html",
            "relUrl": "/git/2021/03/21/Git-Quick-Reference.html",
            "date": " • Mar 21, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "requests module in Python",
            "content": "import requests . d = { &#39;action&#39;: &#39;updateboardmembers&#39;, &#39;user_id&#39;: &#39;539&#39;, &#39;board&#39;: 30, &#39;boardfemale&#39;: 7, &#39;boardcolor&#39;: 1, &#39;boardfemalecolor&#39;: 2 } . r = requests.post(&quot;http://54.149.150.124/webhook&quot;, data=d) print(r.status_code, r.reason) . 200 OK .",
            "url": "https://tanmay-kulkarni.github.io/blog/requests/python/2021/02/22/requests-module-in-python.html",
            "relUrl": "/requests/python/2021/02/22/requests-module-in-python.html",
            "date": " • Feb 22, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "pandas tricks",
            "content": "import pandas as pd . round((int(1)/(int(6))) * 100) . 17 .",
            "url": "https://tanmay-kulkarni.github.io/blog/pandas/python/2021/02/21/pandas-tricks.html",
            "relUrl": "/pandas/python/2021/02/21/pandas-tricks.html",
            "date": " • Feb 21, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Test Blog post",
            "content": "def some_function(a, b): return a*b . .",
            "url": "https://tanmay-kulkarni.github.io/blog/2020/11/01/Test.html",
            "relUrl": "/2020/11/01/Test.html",
            "date": " • Nov 1, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://tanmay-kulkarni.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://tanmay-kulkarni.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://tanmay-kulkarni.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tanmay-kulkarni.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}